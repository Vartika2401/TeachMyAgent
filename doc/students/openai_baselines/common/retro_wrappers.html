<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>TeachMyAgent.students.openai_baselines.common.retro_wrappers API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>TeachMyAgent.students.openai_baselines.common.retro_wrappers</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from collections import deque
import cv2
cv2.ocl.setUseOpenCL(False)
from .atari_wrappers import WarpFrame, ClipRewardEnv, FrameStack, ScaledFloatFrame
from .wrappers import TimeLimit
import numpy as np
import gym


class StochasticFrameSkip(gym.Wrapper):
    def __init__(self, env, n, stickprob):
        gym.Wrapper.__init__(self, env)
        self.n = n
        self.stickprob = stickprob
        self.curac = None
        self.rng = np.random.RandomState()
        self.supports_want_render = hasattr(env, &#34;supports_want_render&#34;)

    def reset(self, **kwargs):
        self.curac = None
        return self.env.reset(**kwargs)

    def step(self, ac):
        done = False
        totrew = 0
        for i in range(self.n):
            # First step after reset, use action
            if self.curac is None:
                self.curac = ac
            # First substep, delay with probability=stickprob
            elif i==0:
                if self.rng.rand() &gt; self.stickprob:
                    self.curac = ac
            # Second substep, new action definitely kicks in
            elif i==1:
                self.curac = ac
            if self.supports_want_render and i&lt;self.n-1:
                ob, rew, done, info = self.env.step(self.curac, want_render=False)
            else:
                ob, rew, done, info = self.env.step(self.curac)
            totrew += rew
            if done: break
        return ob, totrew, done, info

    def seed(self, s):
        self.rng.seed(s)

class PartialFrameStack(gym.Wrapper):
    def __init__(self, env, k, channel=1):
        &#34;&#34;&#34;
        Stack one channel (channel keyword) from previous frames
        &#34;&#34;&#34;
        gym.Wrapper.__init__(self, env)
        shp = env.observation_space.shape
        self.channel = channel
        self.observation_space = gym.spaces.Box(low=0, high=255,
            shape=(shp[0], shp[1], shp[2] + k - 1),
            dtype=env.observation_space.dtype)
        self.k = k
        self.frames = deque([], maxlen=k)
        shp = env.observation_space.shape

    def reset(self):
        ob = self.env.reset()
        assert ob.shape[2] &gt; self.channel
        for _ in range(self.k):
            self.frames.append(ob)
        return self._get_ob()

    def step(self, ac):
        ob, reward, done, info = self.env.step(ac)
        self.frames.append(ob)
        return self._get_ob(), reward, done, info

    def _get_ob(self):
        assert len(self.frames) == self.k
        return np.concatenate([frame if i==self.k-1 else frame[:,:,self.channel:self.channel+1]
            for (i, frame) in enumerate(self.frames)], axis=2)

class Downsample(gym.ObservationWrapper):
    def __init__(self, env, ratio):
        &#34;&#34;&#34;
        Downsample images by a factor of ratio
        &#34;&#34;&#34;
        gym.ObservationWrapper.__init__(self, env)
        (oldh, oldw, oldc) = env.observation_space.shape
        newshape = (oldh//ratio, oldw//ratio, oldc)
        self.observation_space = gym.spaces.Box(low=0, high=255,
            shape=newshape, dtype=np.uint8)

    def observation(self, frame):
        height, width, _ = self.observation_space.shape
        frame = cv2.resize(frame, (width, height), interpolation=cv2.INTER_AREA)
        if frame.ndim == 2:
            frame = frame[:,:,None]
        return frame

class Rgb2gray(gym.ObservationWrapper):
    def __init__(self, env):
        &#34;&#34;&#34;
        Downsample images by a factor of ratio
        &#34;&#34;&#34;
        gym.ObservationWrapper.__init__(self, env)
        (oldh, oldw, _oldc) = env.observation_space.shape
        self.observation_space = gym.spaces.Box(low=0, high=255,
            shape=(oldh, oldw, 1), dtype=np.uint8)

    def observation(self, frame):
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
        return frame[:,:,None]


class MovieRecord(gym.Wrapper):
    def __init__(self, env, savedir, k):
        gym.Wrapper.__init__(self, env)
        self.savedir = savedir
        self.k = k
        self.epcount = 0
    def reset(self):
        if self.epcount % self.k == 0:
            self.env.unwrapped.movie_path = self.savedir
        else:
            self.env.unwrapped.movie_path = None
            self.env.unwrapped.movie = None
        self.epcount += 1
        return self.env.reset()

class AppendTimeout(gym.Wrapper):
    def __init__(self, env):
        gym.Wrapper.__init__(self, env)
        self.action_space = env.action_space
        self.timeout_space = gym.spaces.Box(low=np.array([0.0]), high=np.array([1.0]), dtype=np.float32)
        self.original_os = env.observation_space
        if isinstance(self.original_os, gym.spaces.Dict):
            import copy
            ordered_dict = copy.deepcopy(self.original_os.spaces)
            ordered_dict[&#39;value_estimation_timeout&#39;] = self.timeout_space
            self.observation_space = gym.spaces.Dict(ordered_dict)
            self.dict_mode = True
        else:
            self.observation_space = gym.spaces.Dict({
                &#39;original&#39;: self.original_os,
                &#39;value_estimation_timeout&#39;: self.timeout_space
                })
            self.dict_mode = False
        self.ac_count = None
        while 1:
            if not hasattr(env, &#34;_max_episode_steps&#34;):  # Looking for TimeLimit wrapper that has this field
                env = env.env
                continue
            break
        self.timeout = env._max_episode_steps

    def step(self, ac):
        self.ac_count += 1
        ob, rew, done, info = self.env.step(ac)
        return self._process(ob), rew, done, info

    def reset(self):
        self.ac_count = 0
        return self._process(self.env.reset())

    def _process(self, ob):
        fracmissing = 1 - self.ac_count / self.timeout
        if self.dict_mode:
            ob[&#39;value_estimation_timeout&#39;] = fracmissing
        else:
            return { &#39;original&#39;: ob, &#39;value_estimation_timeout&#39;: fracmissing }

class StartDoingRandomActionsWrapper(gym.Wrapper):
    &#34;&#34;&#34;
    Warning: can eat info dicts, not good if you depend on them
    &#34;&#34;&#34;
    def __init__(self, env, max_random_steps, on_startup=True, every_episode=False):
        gym.Wrapper.__init__(self, env)
        self.on_startup = on_startup
        self.every_episode = every_episode
        self.random_steps = max_random_steps
        self.last_obs = None
        if on_startup:
            self.some_random_steps()

    def some_random_steps(self):
        self.last_obs = self.env.reset()
        n = np.random.randint(self.random_steps)
        #print(&#34;running for random %i frames&#34; % n)
        for _ in range(n):
            self.last_obs, _, done, _ = self.env.step(self.env.action_space.sample())
            if done: self.last_obs = self.env.reset()

    def reset(self):
        return self.last_obs

    def step(self, a):
        self.last_obs, rew, done, info = self.env.step(a)
        if done:
            self.last_obs = self.env.reset()
            if self.every_episode:
                self.some_random_steps()
        return self.last_obs, rew, done, info

def make_retro(*, game, state=None, max_episode_steps=4500, **kwargs):
    import retro
    if state is None:
        state = retro.State.DEFAULT
    env = retro.make(game, state, **kwargs)
    env = StochasticFrameSkip(env, n=4, stickprob=0.25)
    if max_episode_steps is not None:
        env = TimeLimit(env, max_episode_steps=max_episode_steps)
    return env

def wrap_deepmind_retro(env, scale=True, frame_stack=4):
    &#34;&#34;&#34;
    Configure environment for retro games, using config similar to DeepMind-style Atari in wrap_deepmind
    &#34;&#34;&#34;
    env = WarpFrame(env)
    env = ClipRewardEnv(env)
    if frame_stack &gt; 1:
        env = FrameStack(env, frame_stack)
    if scale:
        env = ScaledFloatFrame(env)
    return env

class SonicDiscretizer(gym.ActionWrapper):
    &#34;&#34;&#34;
    Wrap a gym-retro environment and make it use discrete
    actions for the Sonic game.
    &#34;&#34;&#34;
    def __init__(self, env):
        super(SonicDiscretizer, self).__init__(env)
        buttons = [&#34;B&#34;, &#34;A&#34;, &#34;MODE&#34;, &#34;START&#34;, &#34;UP&#34;, &#34;DOWN&#34;, &#34;LEFT&#34;, &#34;RIGHT&#34;, &#34;C&#34;, &#34;Y&#34;, &#34;X&#34;, &#34;Z&#34;]
        actions = [[&#39;LEFT&#39;], [&#39;RIGHT&#39;], [&#39;LEFT&#39;, &#39;DOWN&#39;], [&#39;RIGHT&#39;, &#39;DOWN&#39;], [&#39;DOWN&#39;],
                   [&#39;DOWN&#39;, &#39;B&#39;], [&#39;B&#39;]]
        self._actions = []
        for action in actions:
            arr = np.array([False] * 12)
            for button in action:
                arr[buttons.index(button)] = True
            self._actions.append(arr)
        self.action_space = gym.spaces.Discrete(len(self._actions))

    def action(self, a): # pylint: disable=W0221
        return self._actions[a].copy()

class RewardScaler(gym.RewardWrapper):
    &#34;&#34;&#34;
    Bring rewards to a reasonable scale for PPO.
    This is incredibly important and effects performance
    drastically.
    &#34;&#34;&#34;
    def __init__(self, env, scale=0.01):
        super(RewardScaler, self).__init__(env)
        self.scale = scale

    def reward(self, reward):
        return reward * self.scale

class AllowBacktracking(gym.Wrapper):
    &#34;&#34;&#34;
    Use deltas in max(X) as the reward, rather than deltas
    in X. This way, agents are not discouraged too heavily
    from exploring backwards if there is no way to advance
    head-on in the level.
    &#34;&#34;&#34;
    def __init__(self, env):
        super(AllowBacktracking, self).__init__(env)
        self._cur_x = 0
        self._max_x = 0

    def reset(self, **kwargs): # pylint: disable=E0202
        self._cur_x = 0
        self._max_x = 0
        return self.env.reset(**kwargs)

    def step(self, action): # pylint: disable=E0202
        obs, rew, done, info = self.env.step(action)
        self._cur_x += rew
        rew = max(0, self._cur_x - self._max_x)
        self._max_x = max(self._max_x, self._cur_x)
        return obs, rew, done, info</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.make_retro"><code class="name flex">
<span>def <span class="ident">make_retro</span></span>(<span>*, game, state=None, max_episode_steps=4500, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_retro(*, game, state=None, max_episode_steps=4500, **kwargs):
    import retro
    if state is None:
        state = retro.State.DEFAULT
    env = retro.make(game, state, **kwargs)
    env = StochasticFrameSkip(env, n=4, stickprob=0.25)
    if max_episode_steps is not None:
        env = TimeLimit(env, max_episode_steps=max_episode_steps)
    return env</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.wrap_deepmind_retro"><code class="name flex">
<span>def <span class="ident">wrap_deepmind_retro</span></span>(<span>env, scale=True, frame_stack=4)</span>
</code></dt>
<dd>
<div class="desc"><p>Configure environment for retro games, using config similar to DeepMind-style Atari in wrap_deepmind</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wrap_deepmind_retro(env, scale=True, frame_stack=4):
    &#34;&#34;&#34;
    Configure environment for retro games, using config similar to DeepMind-style Atari in wrap_deepmind
    &#34;&#34;&#34;
    env = WarpFrame(env)
    env = ClipRewardEnv(env)
    if frame_stack &gt; 1:
        env = FrameStack(env, frame_stack)
    if scale:
        env = ScaledFloatFrame(env)
    return env</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.AllowBacktracking"><code class="flex name class">
<span>class <span class="ident">AllowBacktracking</span></span>
<span>(</span><span>env)</span>
</code></dt>
<dd>
<div class="desc"><p>Use deltas in max(X) as the reward, rather than deltas
in X. This way, agents are not discouraged too heavily
from exploring backwards if there is no way to advance
head-on in the level.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AllowBacktracking(gym.Wrapper):
    &#34;&#34;&#34;
    Use deltas in max(X) as the reward, rather than deltas
    in X. This way, agents are not discouraged too heavily
    from exploring backwards if there is no way to advance
    head-on in the level.
    &#34;&#34;&#34;
    def __init__(self, env):
        super(AllowBacktracking, self).__init__(env)
        self._cur_x = 0
        self._max_x = 0

    def reset(self, **kwargs): # pylint: disable=E0202
        self._cur_x = 0
        self._max_x = 0
        return self.env.reset(**kwargs)

    def step(self, action): # pylint: disable=E0202
        obs, rew, done, info = self.env.step(action)
        self._cur_x += rew
        rew = max(0, self._cur_x - self._max_x)
        self._max_x = max(self._max_x, self._cur_x)
        return obs, rew, done, info</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>gym.core.Wrapper</li>
<li>gym.core.Env</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.AllowBacktracking.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets the environment to an initial state and returns an initial
observation.</p>
<p>Note that this function should not reset the environment's random
number generator(s); random variables in the environment's state should
be sampled independently between multiple calls to <code>reset()</code>. In other
words, each call of <code>reset()</code> should yield an environment suitable for
a new episode, independent of previous episodes.</p>
<h2 id="returns">Returns</h2>
<p>observation (object): the initial observation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self, **kwargs): # pylint: disable=E0202
    self._cur_x = 0
    self._max_x = 0
    return self.env.reset(**kwargs)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.AllowBacktracking.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, action)</span>
</code></dt>
<dd>
<div class="desc"><p>Run one timestep of the environment's dynamics. When end of
episode is reached, you are responsible for calling <code>reset()</code>
to reset this environment's state.</p>
<p>Accepts an action and returns a tuple (observation, reward, done, info).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>action</code></strong> :&ensp;<code>object</code></dt>
<dd>an action provided by the agent</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>observation (object): agent's observation of the current environment
reward (float) : amount of reward returned after previous action
done (bool): whether the episode has ended, in which case further step() calls will return undefined results
info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, action): # pylint: disable=E0202
    obs, rew, done, info = self.env.step(action)
    self._cur_x += rew
    rew = max(0, self._cur_x - self._max_x)
    self._max_x = max(self._max_x, self._cur_x)
    return obs, rew, done, info</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.AppendTimeout"><code class="flex name class">
<span>class <span class="ident">AppendTimeout</span></span>
<span>(</span><span>env)</span>
</code></dt>
<dd>
<div class="desc"><p>Wraps the environment to allow a modular transformation.</p>
<p>This class is the base class for all wrappers. The subclass could override
some methods to change the behavior of the original environment without touching the
original code.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Don't forget to call <code>super().__init__(env)</code> if the subclass overrides :meth:<code>__init__</code>.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AppendTimeout(gym.Wrapper):
    def __init__(self, env):
        gym.Wrapper.__init__(self, env)
        self.action_space = env.action_space
        self.timeout_space = gym.spaces.Box(low=np.array([0.0]), high=np.array([1.0]), dtype=np.float32)
        self.original_os = env.observation_space
        if isinstance(self.original_os, gym.spaces.Dict):
            import copy
            ordered_dict = copy.deepcopy(self.original_os.spaces)
            ordered_dict[&#39;value_estimation_timeout&#39;] = self.timeout_space
            self.observation_space = gym.spaces.Dict(ordered_dict)
            self.dict_mode = True
        else:
            self.observation_space = gym.spaces.Dict({
                &#39;original&#39;: self.original_os,
                &#39;value_estimation_timeout&#39;: self.timeout_space
                })
            self.dict_mode = False
        self.ac_count = None
        while 1:
            if not hasattr(env, &#34;_max_episode_steps&#34;):  # Looking for TimeLimit wrapper that has this field
                env = env.env
                continue
            break
        self.timeout = env._max_episode_steps

    def step(self, ac):
        self.ac_count += 1
        ob, rew, done, info = self.env.step(ac)
        return self._process(ob), rew, done, info

    def reset(self):
        self.ac_count = 0
        return self._process(self.env.reset())

    def _process(self, ob):
        fracmissing = 1 - self.ac_count / self.timeout
        if self.dict_mode:
            ob[&#39;value_estimation_timeout&#39;] = fracmissing
        else:
            return { &#39;original&#39;: ob, &#39;value_estimation_timeout&#39;: fracmissing }</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>gym.core.Wrapper</li>
<li>gym.core.Env</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.AppendTimeout.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets the environment to an initial state and returns an initial
observation.</p>
<p>Note that this function should not reset the environment's random
number generator(s); random variables in the environment's state should
be sampled independently between multiple calls to <code>reset()</code>. In other
words, each call of <code>reset()</code> should yield an environment suitable for
a new episode, independent of previous episodes.</p>
<h2 id="returns">Returns</h2>
<p>observation (object): the initial observation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    self.ac_count = 0
    return self._process(self.env.reset())</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.AppendTimeout.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, ac)</span>
</code></dt>
<dd>
<div class="desc"><p>Run one timestep of the environment's dynamics. When end of
episode is reached, you are responsible for calling <code>reset()</code>
to reset this environment's state.</p>
<p>Accepts an action and returns a tuple (observation, reward, done, info).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>action</code></strong> :&ensp;<code>object</code></dt>
<dd>an action provided by the agent</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>observation (object): agent's observation of the current environment
reward (float) : amount of reward returned after previous action
done (bool): whether the episode has ended, in which case further step() calls will return undefined results
info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, ac):
    self.ac_count += 1
    ob, rew, done, info = self.env.step(ac)
    return self._process(ob), rew, done, info</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.Downsample"><code class="flex name class">
<span>class <span class="ident">Downsample</span></span>
<span>(</span><span>env, ratio)</span>
</code></dt>
<dd>
<div class="desc"><p>Wraps the environment to allow a modular transformation.</p>
<p>This class is the base class for all wrappers. The subclass could override
some methods to change the behavior of the original environment without touching the
original code.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Don't forget to call <code>super().__init__(env)</code> if the subclass overrides :meth:<code>__init__</code>.</p>
</div>
<p>Downsample images by a factor of ratio</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Downsample(gym.ObservationWrapper):
    def __init__(self, env, ratio):
        &#34;&#34;&#34;
        Downsample images by a factor of ratio
        &#34;&#34;&#34;
        gym.ObservationWrapper.__init__(self, env)
        (oldh, oldw, oldc) = env.observation_space.shape
        newshape = (oldh//ratio, oldw//ratio, oldc)
        self.observation_space = gym.spaces.Box(low=0, high=255,
            shape=newshape, dtype=np.uint8)

    def observation(self, frame):
        height, width, _ = self.observation_space.shape
        frame = cv2.resize(frame, (width, height), interpolation=cv2.INTER_AREA)
        if frame.ndim == 2:
            frame = frame[:,:,None]
        return frame</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>gym.core.ObservationWrapper</li>
<li>gym.core.Wrapper</li>
<li>gym.core.Env</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.Downsample.observation"><code class="name flex">
<span>def <span class="ident">observation</span></span>(<span>self, frame)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def observation(self, frame):
    height, width, _ = self.observation_space.shape
    frame = cv2.resize(frame, (width, height), interpolation=cv2.INTER_AREA)
    if frame.ndim == 2:
        frame = frame[:,:,None]
    return frame</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.MovieRecord"><code class="flex name class">
<span>class <span class="ident">MovieRecord</span></span>
<span>(</span><span>env, savedir, k)</span>
</code></dt>
<dd>
<div class="desc"><p>Wraps the environment to allow a modular transformation.</p>
<p>This class is the base class for all wrappers. The subclass could override
some methods to change the behavior of the original environment without touching the
original code.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Don't forget to call <code>super().__init__(env)</code> if the subclass overrides :meth:<code>__init__</code>.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MovieRecord(gym.Wrapper):
    def __init__(self, env, savedir, k):
        gym.Wrapper.__init__(self, env)
        self.savedir = savedir
        self.k = k
        self.epcount = 0
    def reset(self):
        if self.epcount % self.k == 0:
            self.env.unwrapped.movie_path = self.savedir
        else:
            self.env.unwrapped.movie_path = None
            self.env.unwrapped.movie = None
        self.epcount += 1
        return self.env.reset()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>gym.core.Wrapper</li>
<li>gym.core.Env</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.MovieRecord.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets the environment to an initial state and returns an initial
observation.</p>
<p>Note that this function should not reset the environment's random
number generator(s); random variables in the environment's state should
be sampled independently between multiple calls to <code>reset()</code>. In other
words, each call of <code>reset()</code> should yield an environment suitable for
a new episode, independent of previous episodes.</p>
<h2 id="returns">Returns</h2>
<p>observation (object): the initial observation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    if self.epcount % self.k == 0:
        self.env.unwrapped.movie_path = self.savedir
    else:
        self.env.unwrapped.movie_path = None
        self.env.unwrapped.movie = None
    self.epcount += 1
    return self.env.reset()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.PartialFrameStack"><code class="flex name class">
<span>class <span class="ident">PartialFrameStack</span></span>
<span>(</span><span>env, k, channel=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Wraps the environment to allow a modular transformation.</p>
<p>This class is the base class for all wrappers. The subclass could override
some methods to change the behavior of the original environment without touching the
original code.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Don't forget to call <code>super().__init__(env)</code> if the subclass overrides :meth:<code>__init__</code>.</p>
</div>
<p>Stack one channel (channel keyword) from previous frames</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PartialFrameStack(gym.Wrapper):
    def __init__(self, env, k, channel=1):
        &#34;&#34;&#34;
        Stack one channel (channel keyword) from previous frames
        &#34;&#34;&#34;
        gym.Wrapper.__init__(self, env)
        shp = env.observation_space.shape
        self.channel = channel
        self.observation_space = gym.spaces.Box(low=0, high=255,
            shape=(shp[0], shp[1], shp[2] + k - 1),
            dtype=env.observation_space.dtype)
        self.k = k
        self.frames = deque([], maxlen=k)
        shp = env.observation_space.shape

    def reset(self):
        ob = self.env.reset()
        assert ob.shape[2] &gt; self.channel
        for _ in range(self.k):
            self.frames.append(ob)
        return self._get_ob()

    def step(self, ac):
        ob, reward, done, info = self.env.step(ac)
        self.frames.append(ob)
        return self._get_ob(), reward, done, info

    def _get_ob(self):
        assert len(self.frames) == self.k
        return np.concatenate([frame if i==self.k-1 else frame[:,:,self.channel:self.channel+1]
            for (i, frame) in enumerate(self.frames)], axis=2)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>gym.core.Wrapper</li>
<li>gym.core.Env</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.PartialFrameStack.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets the environment to an initial state and returns an initial
observation.</p>
<p>Note that this function should not reset the environment's random
number generator(s); random variables in the environment's state should
be sampled independently between multiple calls to <code>reset()</code>. In other
words, each call of <code>reset()</code> should yield an environment suitable for
a new episode, independent of previous episodes.</p>
<h2 id="returns">Returns</h2>
<p>observation (object): the initial observation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    ob = self.env.reset()
    assert ob.shape[2] &gt; self.channel
    for _ in range(self.k):
        self.frames.append(ob)
    return self._get_ob()</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.PartialFrameStack.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, ac)</span>
</code></dt>
<dd>
<div class="desc"><p>Run one timestep of the environment's dynamics. When end of
episode is reached, you are responsible for calling <code>reset()</code>
to reset this environment's state.</p>
<p>Accepts an action and returns a tuple (observation, reward, done, info).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>action</code></strong> :&ensp;<code>object</code></dt>
<dd>an action provided by the agent</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>observation (object): agent's observation of the current environment
reward (float) : amount of reward returned after previous action
done (bool): whether the episode has ended, in which case further step() calls will return undefined results
info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, ac):
    ob, reward, done, info = self.env.step(ac)
    self.frames.append(ob)
    return self._get_ob(), reward, done, info</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.RewardScaler"><code class="flex name class">
<span>class <span class="ident">RewardScaler</span></span>
<span>(</span><span>env, scale=0.01)</span>
</code></dt>
<dd>
<div class="desc"><p>Bring rewards to a reasonable scale for PPO.
This is incredibly important and effects performance
drastically.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RewardScaler(gym.RewardWrapper):
    &#34;&#34;&#34;
    Bring rewards to a reasonable scale for PPO.
    This is incredibly important and effects performance
    drastically.
    &#34;&#34;&#34;
    def __init__(self, env, scale=0.01):
        super(RewardScaler, self).__init__(env)
        self.scale = scale

    def reward(self, reward):
        return reward * self.scale</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>gym.core.RewardWrapper</li>
<li>gym.core.Wrapper</li>
<li>gym.core.Env</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.RewardScaler.reward"><code class="name flex">
<span>def <span class="ident">reward</span></span>(<span>self, reward)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reward(self, reward):
    return reward * self.scale</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.Rgb2gray"><code class="flex name class">
<span>class <span class="ident">Rgb2gray</span></span>
<span>(</span><span>env)</span>
</code></dt>
<dd>
<div class="desc"><p>Wraps the environment to allow a modular transformation.</p>
<p>This class is the base class for all wrappers. The subclass could override
some methods to change the behavior of the original environment without touching the
original code.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Don't forget to call <code>super().__init__(env)</code> if the subclass overrides :meth:<code>__init__</code>.</p>
</div>
<p>Downsample images by a factor of ratio</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Rgb2gray(gym.ObservationWrapper):
    def __init__(self, env):
        &#34;&#34;&#34;
        Downsample images by a factor of ratio
        &#34;&#34;&#34;
        gym.ObservationWrapper.__init__(self, env)
        (oldh, oldw, _oldc) = env.observation_space.shape
        self.observation_space = gym.spaces.Box(low=0, high=255,
            shape=(oldh, oldw, 1), dtype=np.uint8)

    def observation(self, frame):
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
        return frame[:,:,None]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>gym.core.ObservationWrapper</li>
<li>gym.core.Wrapper</li>
<li>gym.core.Env</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.Rgb2gray.observation"><code class="name flex">
<span>def <span class="ident">observation</span></span>(<span>self, frame)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def observation(self, frame):
    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
    return frame[:,:,None]</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.SonicDiscretizer"><code class="flex name class">
<span>class <span class="ident">SonicDiscretizer</span></span>
<span>(</span><span>env)</span>
</code></dt>
<dd>
<div class="desc"><p>Wrap a gym-retro environment and make it use discrete
actions for the Sonic game.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SonicDiscretizer(gym.ActionWrapper):
    &#34;&#34;&#34;
    Wrap a gym-retro environment and make it use discrete
    actions for the Sonic game.
    &#34;&#34;&#34;
    def __init__(self, env):
        super(SonicDiscretizer, self).__init__(env)
        buttons = [&#34;B&#34;, &#34;A&#34;, &#34;MODE&#34;, &#34;START&#34;, &#34;UP&#34;, &#34;DOWN&#34;, &#34;LEFT&#34;, &#34;RIGHT&#34;, &#34;C&#34;, &#34;Y&#34;, &#34;X&#34;, &#34;Z&#34;]
        actions = [[&#39;LEFT&#39;], [&#39;RIGHT&#39;], [&#39;LEFT&#39;, &#39;DOWN&#39;], [&#39;RIGHT&#39;, &#39;DOWN&#39;], [&#39;DOWN&#39;],
                   [&#39;DOWN&#39;, &#39;B&#39;], [&#39;B&#39;]]
        self._actions = []
        for action in actions:
            arr = np.array([False] * 12)
            for button in action:
                arr[buttons.index(button)] = True
            self._actions.append(arr)
        self.action_space = gym.spaces.Discrete(len(self._actions))

    def action(self, a): # pylint: disable=W0221
        return self._actions[a].copy()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>gym.core.ActionWrapper</li>
<li>gym.core.Wrapper</li>
<li>gym.core.Env</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.SonicDiscretizer.action"><code class="name flex">
<span>def <span class="ident">action</span></span>(<span>self, a)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def action(self, a): # pylint: disable=W0221
    return self._actions[a].copy()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.StartDoingRandomActionsWrapper"><code class="flex name class">
<span>class <span class="ident">StartDoingRandomActionsWrapper</span></span>
<span>(</span><span>env, max_random_steps, on_startup=True, every_episode=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Warning: can eat info dicts, not good if you depend on them</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class StartDoingRandomActionsWrapper(gym.Wrapper):
    &#34;&#34;&#34;
    Warning: can eat info dicts, not good if you depend on them
    &#34;&#34;&#34;
    def __init__(self, env, max_random_steps, on_startup=True, every_episode=False):
        gym.Wrapper.__init__(self, env)
        self.on_startup = on_startup
        self.every_episode = every_episode
        self.random_steps = max_random_steps
        self.last_obs = None
        if on_startup:
            self.some_random_steps()

    def some_random_steps(self):
        self.last_obs = self.env.reset()
        n = np.random.randint(self.random_steps)
        #print(&#34;running for random %i frames&#34; % n)
        for _ in range(n):
            self.last_obs, _, done, _ = self.env.step(self.env.action_space.sample())
            if done: self.last_obs = self.env.reset()

    def reset(self):
        return self.last_obs

    def step(self, a):
        self.last_obs, rew, done, info = self.env.step(a)
        if done:
            self.last_obs = self.env.reset()
            if self.every_episode:
                self.some_random_steps()
        return self.last_obs, rew, done, info</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>gym.core.Wrapper</li>
<li>gym.core.Env</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.StartDoingRandomActionsWrapper.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets the environment to an initial state and returns an initial
observation.</p>
<p>Note that this function should not reset the environment's random
number generator(s); random variables in the environment's state should
be sampled independently between multiple calls to <code>reset()</code>. In other
words, each call of <code>reset()</code> should yield an environment suitable for
a new episode, independent of previous episodes.</p>
<h2 id="returns">Returns</h2>
<p>observation (object): the initial observation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    return self.last_obs</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.StartDoingRandomActionsWrapper.some_random_steps"><code class="name flex">
<span>def <span class="ident">some_random_steps</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def some_random_steps(self):
    self.last_obs = self.env.reset()
    n = np.random.randint(self.random_steps)
    #print(&#34;running for random %i frames&#34; % n)
    for _ in range(n):
        self.last_obs, _, done, _ = self.env.step(self.env.action_space.sample())
        if done: self.last_obs = self.env.reset()</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.StartDoingRandomActionsWrapper.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, a)</span>
</code></dt>
<dd>
<div class="desc"><p>Run one timestep of the environment's dynamics. When end of
episode is reached, you are responsible for calling <code>reset()</code>
to reset this environment's state.</p>
<p>Accepts an action and returns a tuple (observation, reward, done, info).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>action</code></strong> :&ensp;<code>object</code></dt>
<dd>an action provided by the agent</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>observation (object): agent's observation of the current environment
reward (float) : amount of reward returned after previous action
done (bool): whether the episode has ended, in which case further step() calls will return undefined results
info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, a):
    self.last_obs, rew, done, info = self.env.step(a)
    if done:
        self.last_obs = self.env.reset()
        if self.every_episode:
            self.some_random_steps()
    return self.last_obs, rew, done, info</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.StochasticFrameSkip"><code class="flex name class">
<span>class <span class="ident">StochasticFrameSkip</span></span>
<span>(</span><span>env, n, stickprob)</span>
</code></dt>
<dd>
<div class="desc"><p>Wraps the environment to allow a modular transformation.</p>
<p>This class is the base class for all wrappers. The subclass could override
some methods to change the behavior of the original environment without touching the
original code.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Don't forget to call <code>super().__init__(env)</code> if the subclass overrides :meth:<code>__init__</code>.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class StochasticFrameSkip(gym.Wrapper):
    def __init__(self, env, n, stickprob):
        gym.Wrapper.__init__(self, env)
        self.n = n
        self.stickprob = stickprob
        self.curac = None
        self.rng = np.random.RandomState()
        self.supports_want_render = hasattr(env, &#34;supports_want_render&#34;)

    def reset(self, **kwargs):
        self.curac = None
        return self.env.reset(**kwargs)

    def step(self, ac):
        done = False
        totrew = 0
        for i in range(self.n):
            # First step after reset, use action
            if self.curac is None:
                self.curac = ac
            # First substep, delay with probability=stickprob
            elif i==0:
                if self.rng.rand() &gt; self.stickprob:
                    self.curac = ac
            # Second substep, new action definitely kicks in
            elif i==1:
                self.curac = ac
            if self.supports_want_render and i&lt;self.n-1:
                ob, rew, done, info = self.env.step(self.curac, want_render=False)
            else:
                ob, rew, done, info = self.env.step(self.curac)
            totrew += rew
            if done: break
        return ob, totrew, done, info

    def seed(self, s):
        self.rng.seed(s)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>gym.core.Wrapper</li>
<li>gym.core.Env</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.StochasticFrameSkip.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Resets the environment to an initial state and returns an initial
observation.</p>
<p>Note that this function should not reset the environment's random
number generator(s); random variables in the environment's state should
be sampled independently between multiple calls to <code>reset()</code>. In other
words, each call of <code>reset()</code> should yield an environment suitable for
a new episode, independent of previous episodes.</p>
<h2 id="returns">Returns</h2>
<p>observation (object): the initial observation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self, **kwargs):
    self.curac = None
    return self.env.reset(**kwargs)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.StochasticFrameSkip.seed"><code class="name flex">
<span>def <span class="ident">seed</span></span>(<span>self, s)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets the seed for this env's random number generator(s).</p>
<h2 id="note">Note</h2>
<p>Some environments use multiple pseudorandom number generators.
We want to capture all such seeds used in order to ensure that
there aren't accidental correlations between multiple generators.</p>
<h2 id="returns">Returns</h2>
<p>list<bigint>: Returns the list of seeds used in this env's random
number generators. The first value in the list should be the
"main" seed, or the value which a reproducer should pass to
'seed'. Often, the main seed equals the provided 'seed', but
this won't be true if seed=None, for example.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def seed(self, s):
    self.rng.seed(s)</code></pre>
</details>
</dd>
<dt id="TeachMyAgent.students.openai_baselines.common.retro_wrappers.StochasticFrameSkip.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, ac)</span>
</code></dt>
<dd>
<div class="desc"><p>Run one timestep of the environment's dynamics. When end of
episode is reached, you are responsible for calling <code>reset()</code>
to reset this environment's state.</p>
<p>Accepts an action and returns a tuple (observation, reward, done, info).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>action</code></strong> :&ensp;<code>object</code></dt>
<dd>an action provided by the agent</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>observation (object): agent's observation of the current environment
reward (float) : amount of reward returned after previous action
done (bool): whether the episode has ended, in which case further step() calls will return undefined results
info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, ac):
    done = False
    totrew = 0
    for i in range(self.n):
        # First step after reset, use action
        if self.curac is None:
            self.curac = ac
        # First substep, delay with probability=stickprob
        elif i==0:
            if self.rng.rand() &gt; self.stickprob:
                self.curac = ac
        # Second substep, new action definitely kicks in
        elif i==1:
            self.curac = ac
        if self.supports_want_render and i&lt;self.n-1:
            ob, rew, done, info = self.env.step(self.curac, want_render=False)
        else:
            ob, rew, done, info = self.env.step(self.curac)
        totrew += rew
        if done: break
    return ob, totrew, done, info</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<a href="http://developmentalsystems.org/TeachMyAgent/doc/">
<img src="https://github.com/flowersteam/TeachMyAgent/blob/gh-pages/images/home/head_image.png?raw=true" style="display: block; margin: 1em auto">
</a>
<a href="http://developmentalsystems.org/TeachMyAgent/doc/">Home</a> | <a href="http://developmentalsystems.org/TeachMyAgent/">Website</a>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="TeachMyAgent.students.openai_baselines.common" href="index.html">TeachMyAgent.students.openai_baselines.common</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.make_retro" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.make_retro">make_retro</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.wrap_deepmind_retro" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.wrap_deepmind_retro">wrap_deepmind_retro</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.AllowBacktracking" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.AllowBacktracking">AllowBacktracking</a></code></h4>
<ul class="">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.AllowBacktracking.reset" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.AllowBacktracking.reset">reset</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.AllowBacktracking.step" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.AllowBacktracking.step">step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.AppendTimeout" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.AppendTimeout">AppendTimeout</a></code></h4>
<ul class="">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.AppendTimeout.reset" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.AppendTimeout.reset">reset</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.AppendTimeout.step" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.AppendTimeout.step">step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.Downsample" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.Downsample">Downsample</a></code></h4>
<ul class="">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.Downsample.observation" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.Downsample.observation">observation</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.MovieRecord" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.MovieRecord">MovieRecord</a></code></h4>
<ul class="">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.MovieRecord.reset" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.MovieRecord.reset">reset</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.PartialFrameStack" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.PartialFrameStack">PartialFrameStack</a></code></h4>
<ul class="">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.PartialFrameStack.reset" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.PartialFrameStack.reset">reset</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.PartialFrameStack.step" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.PartialFrameStack.step">step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.RewardScaler" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.RewardScaler">RewardScaler</a></code></h4>
<ul class="">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.RewardScaler.reward" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.RewardScaler.reward">reward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.Rgb2gray" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.Rgb2gray">Rgb2gray</a></code></h4>
<ul class="">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.Rgb2gray.observation" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.Rgb2gray.observation">observation</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.SonicDiscretizer" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.SonicDiscretizer">SonicDiscretizer</a></code></h4>
<ul class="">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.SonicDiscretizer.action" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.SonicDiscretizer.action">action</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.StartDoingRandomActionsWrapper" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.StartDoingRandomActionsWrapper">StartDoingRandomActionsWrapper</a></code></h4>
<ul class="">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.StartDoingRandomActionsWrapper.reset" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.StartDoingRandomActionsWrapper.reset">reset</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.StartDoingRandomActionsWrapper.some_random_steps" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.StartDoingRandomActionsWrapper.some_random_steps">some_random_steps</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.StartDoingRandomActionsWrapper.step" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.StartDoingRandomActionsWrapper.step">step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.StochasticFrameSkip" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.StochasticFrameSkip">StochasticFrameSkip</a></code></h4>
<ul class="">
<li><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.StochasticFrameSkip.reset" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.StochasticFrameSkip.reset">reset</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.StochasticFrameSkip.seed" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.StochasticFrameSkip.seed">seed</a></code></li>
<li><code><a title="TeachMyAgent.students.openai_baselines.common.retro_wrappers.StochasticFrameSkip.step" href="#TeachMyAgent.students.openai_baselines.common.retro_wrappers.StochasticFrameSkip.step">step</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>